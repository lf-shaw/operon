{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set(context='notebook', font_scale=1.5, style='white', rc={'axes.facecolor':'#fcfcfc', 'figure.facecolor':'#fcfcfc'})\n",
    "from scipy import stats\n",
    "import random, time, sys, os, json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up genetic operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tournament_selection(pop, group_size=5):\n",
    "    s = len(pop)\n",
    "    i = int(random.random() * s)\n",
    "    for _ in range(1, group_size):\n",
    "        j = int(random.random() * s)\n",
    "        if pop[i][1] < pop[j][1]:\n",
    "            i = j\n",
    "        \n",
    "    return i\n",
    "\n",
    "def proportional_selection(pop, cum):\n",
    "    c = random.random() * cum[-1]\n",
    "    for i, p in enumerate(pop):\n",
    "        if c < cum[i]:\n",
    "            return i\n",
    "\n",
    "def get_best(pop):\n",
    "    f = lambda x: x[1][1]\n",
    "    return max(enumerate(p for p in pop if p is not None), key=f)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gp(data_path, run_index, algorithm_name, population_size, max_evaluations, tournament_size, p_crossover, p_mutation):\n",
    "    base_path = os.path.dirname(data_path)\n",
    "    \n",
    "    with open(data_path, 'r') as h:\n",
    "        info           = json.load(h)\n",
    "        metadata       = info['metadata']\n",
    "        target         = metadata['target']\n",
    "        training_rows  = metadata['training_rows']\n",
    "        training_start = training_rows['start']\n",
    "        training_end   = training_rows['end']\n",
    "        test_rows      = metadata['test_rows']\n",
    "        test_start     = test_rows['start']\n",
    "        test_end       = test_rows['end']\n",
    "        problem_name   = metadata['name']\n",
    "        problem_csv    = metadata['filename']\n",
    "        \n",
    "        datafile       = os.path.join(base_path, problem_csv)\n",
    "    \n",
    "        import _operon as Operon # operon python bindings\n",
    "        ds             = Operon.Dataset(datafile, has_header=True)\n",
    "        training_range = Operon.Range(training_start, training_end)\n",
    "        test_range     = Operon.Range(test_start, test_end)\n",
    "        target         = ds.GetVariable('Y')\n",
    "        inputs         = Operon.VariableCollection(v for v in ds.Variables if v.Name != target.Name)\n",
    "\n",
    "        y_train        = ds.Values[training_start:training_end, target.Index]\n",
    "\n",
    "        rng            = Operon.RomuTrio(random.randint(1, 1000000))\n",
    "        # some more common parameters\n",
    "        min_length, max_length = 1, 50\n",
    "        min_depth, max_depth   = 1, 10\n",
    "        p_internal = 0.9 # crossover\n",
    "        \n",
    "        pset           = Operon.PrimitiveSet()\n",
    "        pset.SetConfig(Operon.PrimitiveSet.Arithmetic | Operon.NodeType.Exp | Operon.NodeType.Log | Operon.NodeType.Sin | Operon.NodeType.Cos)\n",
    "        \n",
    "        # tree creator\n",
    "        initial_lengths = np.random.randint(min_length, max_length+1, population_size)\n",
    "        btc             = Operon.BalancedTreeCreator(pset, inputs, bias=0.0)\n",
    "        grow            = Operon.GrowTreeCreator(pset, inputs)\n",
    "\n",
    "        # crossover\n",
    "        crossover      = Operon.SubtreeCrossover(p_internal, max_depth, max_length)\n",
    "\n",
    "        # mutation\n",
    "        mut_onepoint   = Operon.OnePointMutation()\n",
    "        mut_changeVar  = Operon.ChangeVariableMutation(inputs)\n",
    "        mut_changeFunc = Operon.ChangeFunctionMutation(pset)\n",
    "        mut_replace    = Operon.ReplaceSubtreeMutation(btc, max_depth, max_length)\n",
    "        mutation       = [ mut_onepoint, mut_changeFunc, mut_replace, mut_changeVar ]\n",
    "\n",
    "        # offspring generator (applies selection, crossover and mutation)\n",
    "        # to create one offspring individual\n",
    "        # this is the way a GP algorithm usually defines recombination\n",
    "        def generate(pop):\n",
    "            do_crossover = random.uniform(0, 1) < p_crossover\n",
    "            do_mutation = random.uniform(0, 1) < p_mutation\n",
    "\n",
    "            i1 = tournament_selection(pop, tournament_size)\n",
    "            p1, f1 = pop[i1]\n",
    "\n",
    "            if do_crossover:\n",
    "                i2 = tournament_selection(pop, tournament_size)\n",
    "                p2, f2 = pop[i2]\n",
    "                child = crossover(rng, p1, p2)\n",
    "\n",
    "            if do_mutation:\n",
    "                op = random.choice(mutation)\n",
    "                p1 = child if do_crossover else p1\n",
    "                child = op(rng, child)\n",
    "\n",
    "            if do_crossover or do_mutation:\n",
    "                return child, Operon.CalculateFitness(child, ds, training_range, target.Name, metric='rsquared')\n",
    "            else:\n",
    "                return p1,f1\n",
    "\n",
    "        t0 = time.time()\n",
    "        trees = [btc(rng, l, 0, 0) for l in initial_lengths]\n",
    "        fit = [Operon.CalculateFitness(t, ds, training_range, target.Name, metric='rsquared') for t in trees]\n",
    "        pop = [p for p in zip(trees, fit)]\n",
    "        best = get_best(pop)\n",
    "        \n",
    "        r2_test = Operon.CalculateFitness(pop[best][0], ds, test_range, target.Name, metric='rsquared')\n",
    "        \n",
    "        evaluations = population_size\n",
    "\n",
    "        df = pd.DataFrame(columns=['Algorithm', 'Run index', 'Generation', 'Problem', 'R2 train', 'R2 test', 'Best quality', 'Avg quality', 'Avg length', 'Evaluated solutions'])\n",
    "        \n",
    "        row = [\n",
    "            algorithm_name,\n",
    "            run_index,\n",
    "            0,\n",
    "            problem_name,\n",
    "            pop[best][1],\n",
    "            r2_test,\n",
    "            pop[best][1],\n",
    "            np.mean([p[1] for p in pop]),\n",
    "            np.mean([p[0].Length for p in pop]),\n",
    "            evaluations\n",
    "        ]\n",
    "        df.loc[0] = row\n",
    "        \n",
    "        gen = 1\n",
    "        while True:\n",
    "            remaining_budget = min(population_size, max_evaluations - evaluations)\n",
    "            \n",
    "            if remaining_budget <= 0:\n",
    "                break\n",
    "            \n",
    "            pop = [pop[best] if i == best else generate(pop) for i in range(remaining_budget)]\n",
    "            new_best = get_best(pop)\n",
    "            \n",
    "            evaluations += remaining_budget \n",
    "         \n",
    "            if new_best != best:\n",
    "                best = new_best\n",
    "                r2_test = Operon.CalculateFitness(pop[best][0], ds, test_range, target.Name, metric='rsquared')\n",
    "\n",
    "            row = [\n",
    "                algorithm_name,\n",
    "                run_index,\n",
    "                gen,\n",
    "                problem_name,\n",
    "                pop[best][1],\n",
    "                r2_test,\n",
    "                pop[best][1],\n",
    "                np.mean([p[1] for p in pop]),\n",
    "                np.mean([p[0].Length for p in pop]),\n",
    "                evaluations\n",
    "            ]\n",
    "            df.loc[gen] = row\n",
    "            \n",
    "            gen += 1\n",
    "\n",
    "        t1 = time.time()\n",
    "        return df\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offspring selection GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_osgp(data_path, run_index, algorithm_name, population_size, max_evaluations, max_selection_pressure, tournament_size, p_crossover, p_mutation):\n",
    "    base_path = os.path.dirname(data_path)\n",
    "    \n",
    "    with open(data_path, 'r') as h:\n",
    "        info           = json.load(h)\n",
    "        metadata       = info['metadata']\n",
    "        target         = metadata['target']\n",
    "        training_rows  = metadata['training_rows']\n",
    "        training_start = training_rows['start']\n",
    "        training_end   = training_rows['end']\n",
    "        test_rows      = metadata['test_rows']\n",
    "        test_start     = test_rows['start']\n",
    "        test_end       = test_rows['end']\n",
    "        problem_name   = metadata['name']\n",
    "        problem_csv    = metadata['filename']\n",
    "        \n",
    "        datafile       = os.path.join(base_path, problem_csv)\n",
    "    \n",
    "        import _operon as Operon # operon python bindings\n",
    "        ds             = Operon.Dataset(datafile, has_header=True)\n",
    "        training_range = Operon.Range(training_start, training_end)\n",
    "        test_range     = Operon.Range(test_start, test_end)\n",
    "        target         = ds.GetVariable('Y')\n",
    "        inputs         = Operon.VariableCollection(v for v in ds.Variables if v.Name != target.Name)\n",
    "\n",
    "        y_train        = ds.Values[training_start:training_end, target.Index]\n",
    "\n",
    "        rng            = Operon.RomuTrio(random.randint(1, 1000000))\n",
    "        # some more common parameters\n",
    "        min_length, max_length = 1, 50\n",
    "        min_depth, max_depth   = 1, 10\n",
    "        p_internal = 0.9 # crossover\n",
    "        \n",
    "        pset        = Operon.PrimitiveSet()\n",
    "        pset.SetConfig(Operon.PrimitiveSet.Arithmetic | Operon.NodeType.Exp | Operon.NodeType.Log | Operon.NodeType.Sin | Operon.NodeType.Cos)\n",
    "        \n",
    "        # tree creator\n",
    "        initial_lengths = np.random.randint(min_length, max_length+1, population_size)\n",
    "        btc             = Operon.BalancedTreeCreator(pset, inputs, bias=0.0)\n",
    "        grow            = Operon.GrowTreeCreator(pset, inputs)\n",
    "\n",
    "        # crossover\n",
    "        crossover      = Operon.SubtreeCrossover(p_internal, max_depth, max_length)\n",
    "\n",
    "        # mutation\n",
    "        mut_onepoint   = Operon.OnePointMutation()\n",
    "        mut_changeVar  = Operon.ChangeVariableMutation(inputs)\n",
    "        mut_changeFunc = Operon.ChangeFunctionMutation(pset)\n",
    "        mut_replace    = Operon.ReplaceSubtreeMutation(btc, max_depth, max_length)\n",
    "        mutation       = [ mut_onepoint, mut_changeFunc, mut_replace, mut_changeVar ]\n",
    "\n",
    "        generated = 0    \n",
    "        cumsum = [0] * population_size\n",
    "\n",
    "        def generate(pop, comparison_factor=0):\n",
    "            nonlocal generated\n",
    "            nonlocal cumsum\n",
    "            \n",
    "            if generated / population_size > max_selection_pressure:\n",
    "                return None\n",
    "            \n",
    "            while True:\n",
    "                generated += 1\n",
    "\n",
    "                do_crossover = random.uniform(0, 1) < p_crossover\n",
    "                do_mutation = random.uniform(0, 1) < p_mutation\n",
    "\n",
    "                i1 = proportional_selection(pop, cumsum)\n",
    "#                 i1 = tournament_selection(pop, 2)\n",
    "#                 i1 = random.randrange(population_size)\n",
    "                p1, f1 = pop[i1]\n",
    "                f2 = 0\n",
    "\n",
    "                if do_crossover:\n",
    "                    i2 = random.randrange(population_size)\n",
    "                    p2, f2 = pop[i2]\n",
    "                    child = crossover(rng, p1, p2)\n",
    "\n",
    "                if do_mutation:\n",
    "                    op = random.choice(mutation)\n",
    "                    p1 = child if do_crossover else p1\n",
    "                    child = op(rng, child)\n",
    "\n",
    "                if do_crossover or do_mutation:\n",
    "                    fc = Operon.CalculateFitness(child, ds, training_range, target.Name, metric='rsquared')\n",
    "\n",
    "                    if fc > min(f1, f2) + comparison_factor * abs(f1-f2):\n",
    "                        return child, fc\n",
    "\n",
    "                if not generated / population_size < max_selection_pressure:\n",
    "                    break\n",
    "\n",
    "            return None\n",
    "    \n",
    "        t0 = time.time()\n",
    "        trees = [btc(rng, l, 0, 0) for l in initial_lengths]\n",
    "        fit = [Operon.CalculateFitness(t, ds, training_range, target.Name, metric='rsquared') for t in trees]\n",
    "        pop = [p for p in zip(trees, fit)]\n",
    "        best = get_best(pop)\n",
    "        r2_test = Operon.CalculateFitness(pop[best][0], ds, test_range, target.Name, metric='rsquared')\n",
    "        \n",
    "        evaluations = population_size\n",
    "\n",
    "        df = pd.DataFrame(columns=['Algorithm', 'Run index', 'Generation', 'Problem', 'R2 train', 'R2 test', 'Best quality', 'Avg quality', 'Avg length', 'Evaluated solutions'])\n",
    "        \n",
    "        row = [\n",
    "            algorithm_name,\n",
    "            run_index,\n",
    "            0,\n",
    "            problem_name,\n",
    "            pop[best][1],\n",
    "            r2_test,\n",
    "            pop[best][1],\n",
    "            np.mean([p[1] for p in pop]),\n",
    "            np.mean([p[0].Length for p in pop]),\n",
    "            evaluations\n",
    "        ]\n",
    "        df.loc[0] = row\n",
    "\n",
    "        gen = 1\n",
    "        while True:\n",
    "            remaining_budget = min(population_size, max_evaluations - evaluations)\n",
    "            \n",
    "            if remaining_budget <= 0:\n",
    "                break\n",
    "            \n",
    "            # precalculate the cummulative fitness sum, for proportional selection\n",
    "            c = 0\n",
    "            for i, p in enumerate(pop):\n",
    "                c += pop[i][1]\n",
    "                cumsum[i] = c\n",
    "                \n",
    "            pop = [pop[best] if i == best else generate(pop, 1.0) for i in range(remaining_budget)]\n",
    "            new_best = get_best(pop)\n",
    "            \n",
    "            selection_pressure = generated / population_size\n",
    "            evaluations += generated\n",
    "            generated = 0\n",
    "            \n",
    "            if new_best is None:\n",
    "                break\n",
    "\n",
    "            if new_best != best:\n",
    "                best = new_best\n",
    "                r2_test = Operon.CalculateFitness(pop[best][0], ds, test_range, target.Name, metric='rsquared')\n",
    "\n",
    "            row = [\n",
    "                algorithm_name,\n",
    "                run_index,\n",
    "                gen,\n",
    "                problem_name,\n",
    "                pop[best][1],\n",
    "                r2_test,\n",
    "                pop[best][1],\n",
    "                np.mean([p[1] for p in pop if p is not None]),\n",
    "                np.mean([p[0].Length for p in pop if p is not None]),\n",
    "                evaluations\n",
    "            ]\n",
    "            df.loc[gen] = row\n",
    "            gen += 1\n",
    "            \n",
    "            if selection_pressure > max_selection_pressure or evaluations > max_evaluations:\n",
    "                break\n",
    "\n",
    "        t1 = time.time()\n",
    "        return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    from multiprocessing import Pool, Manager\n",
    "    manager = Manager()\n",
    "    results = manager.list()\n",
    "    \n",
    "    def runner_gp(i):\n",
    "        res = run_gp(data_path='../data/Poly-10.json', run_index=i, algorithm_name='GP', population_size=1000, max_evaluations=1000 * 1000, \n",
    "                     tournament_size=5, p_crossover=1.0, p_mutation=0.25)\n",
    "        results.append(res)\n",
    "        \n",
    "    def runner_osgp(i):\n",
    "        res = run_osgp(data_path='../data/Poly-10.json', run_index=i, algorithm_name='OSGP', population_size=1000, max_evaluations=1000 * 1000, max_selection_pressure=100, \n",
    "                     tournament_size=5, p_crossover=1.0, p_mutation=0.25)\n",
    "        results.append(res)\n",
    "    \n",
    "    pool = Pool()\n",
    "    reps = 10\n",
    "    pool.map(runner_gp, range(reps))\n",
    "    print('GP done')\n",
    "    pool.map(runner_osgp, range(reps))\n",
    "    print('OSGP done')\n",
    "    df = pd.concat(results)\n",
    "    \n",
    "#     print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import seaborn as sns\n",
    "    from matplotlib import rc\n",
    "    import matplotlib.pyplot as plt\n",
    "    sns.set(context='notebook', font_scale=2, style='white')\n",
    "    rc('text', usetex=True)\n",
    "    rc('text.latex', preamble=r'\\usepackage{lmodern}')\n",
    "    from scipy.signal import savgol_filter\n",
    "    \n",
    "    print(df.groupby(['Algorithm']).mean())\n",
    "    print(df.groupby(['Algorithm']).median())\n",
    "    \n",
    "    df_gp = df[df['Algorithm'] == 'GP']\n",
    "    df_osgp = df[df['Algorithm'] == 'OSGP']\n",
    "#     print(df_osgp.head)\n",
    "    intervals = np.zeros(10)\n",
    "    df_last = df_osgp.groupby(['Run index']).last()\n",
    "    for v in df_last['R2 train']:\n",
    "        i = int(v * 10)\n",
    "        for j in range(i+1):\n",
    "            intervals[j] += 1\n",
    "    print(intervals)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def round_down(v, p=3):\n",
    "        n = 10 ** p\n",
    "        return v // n * n\n",
    "        \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))    \n",
    "    sns.lineplot(data=df_gp, x='Evaluated solutions', y='R2 train', ax=ax[0], ci=None)\n",
    "    ax[0].set(xlabel='Evaluated solutions', ylim=(0,1))\n",
    "    sns.lineplot(data=df_gp, x='Evaluated solutions', y='R2 test', ax=ax[1], ci=None)\n",
    "    ax[1].set(xlabel='Evaluated solutions', ylim=(0,1))\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))\n",
    "    sns.lineplot(data=df_gp, x='Evaluated solutions', y='Avg quality', ax=ax[0], ci=None)\n",
    "    sns.lineplot(data=df_gp, x='Evaluated solutions', y='Avg length', ax=ax[1], ci=None)\n",
    "    ax[1].set(xlabel='Evaluated solutions', ylim=(0, 50))\n",
    "    ax[1].set_yticks(np.arange(0, 50, 10))\n",
    "    # for OSGP we want to do some smoothing of the values\n",
    "#     filtered =  savgol_filter(df['R2 train'], 31, 3)\n",
    "    \n",
    "    df_osgp.loc[:, ('Evaluated solutions rounded')] = df_osgp['Evaluated solutions'].apply(lambda x: round_down(x, 4))\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))    \n",
    "    sns.lineplot(data=df_osgp, x='Evaluated solutions rounded', y='R2 train', ax=ax[0], ci=None)\n",
    "    ax[0].set(xlabel='Evaluated solutions')\n",
    "    sns.lineplot(data=df_osgp, x='Evaluated solutions rounded', y='R2 test', ax=ax[1], ci=None)\n",
    "    ax[1].set(xlabel='Evaluated solutions')\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,4))\n",
    "    sns.lineplot(data=df_osgp, x='Evaluated solutions rounded', y='Avg quality', ax=ax[0], ci=None)\n",
    "    sns.lineplot(data=df_osgp, x='Evaluated solutions rounded', y='Avg length', ax=ax[1], ci=None)\n",
    "    ax[1].set(xlabel='Evaluated solutions', ylim=(0, 50))\n",
    "    ax[1].set_yticks(np.arange(0, 50, 10))\n",
    "#     fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
